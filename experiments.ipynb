{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 6\n",
    "\n",
    "## Running the experiments\n",
    "\n",
    "In the previous stage, our team selected `Sammon's error` function and `Genetic algorithm` and `Simulated annealing` for the experiments, so we wil use them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('combined-metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Name', axis=1).values\n",
    "y = df['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 38)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42625369e+00, 3.77802360e+01, 2.97935103e+01, ...,\n",
       "        1.81400000e+03, 1.58500000e+04, 0.00000000e+00],\n",
       "       [2.02732240e+00, 9.52868852e+00, 6.37568306e+00, ...,\n",
       "        9.22000000e+02, 2.64500000e+03, 0.00000000e+00],\n",
       "       [1.67987805e+00, 1.17048780e+01, 8.17073171e+00, ...,\n",
       "        2.79100000e+03, 1.14790000e+04, 0.00000000e+00],\n",
       "       ...,\n",
       "       [4.29198473e+00, 2.41488550e+01, 1.87232824e+01, ...,\n",
       "        3.82000000e+02, 6.43600000e+03, 0.00000000e+00],\n",
       "       [9.15789474e+00, 3.77017544e+01, 2.96842105e+01, ...,\n",
       "        1.78000000e+02, 1.38800000e+03, 0.00000000e+00],\n",
       "       [2.24528302e+00, 1.04371069e+01, 7.09119497e+00, ...,\n",
       "        8.01000000e+02, 2.98200000e+03, 0.00000000e+00]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ART', 'BentoML', 'Bokeh', 'Camel', 'CatBoost', 'Causal ML',\n",
       "       'Chainer', 'Computer Vision', 'D2L', 'Darts', 'Dash', 'DeepChem',\n",
       "       'DeepMind Control', 'DeepPavlov', 'Detectron2', 'DIGITS',\n",
       "       'DragGan', 'EasyOCR', 'ELI5', 'EvalAI', 'facenet', 'FaceSwap',\n",
       "       'Fairseq', 'FastAI', 'FeatureTools', 'FiftyOne', 'gensim',\n",
       "       'Giskard', 'Gluonts', 'Google Flax', 'Google JAX', 'GPT-Engineer',\n",
       "       'GPTDiscord', 'Gradio', 'Gymnasium', 'Horovod', 'ImageAI',\n",
       "       'imbalanced-learn', 'InsightFace', 'Kaolin', 'Kedro', 'Keras',\n",
       "       'Kserve', 'Lightning', 'Ludwig', 'Mage-ai', 'Mars', 'Matplotlib',\n",
       "       'metatransformer', 'Mindsdb', 'MLflow', 'Mycroft',\n",
       "       'Neural Prophet', 'NNI', 'Numpy', 'ONNX', 'Open-Assistant',\n",
       "       'OpenAI Baselines', 'OpenAI Python API library', 'OpenVINO',\n",
       "       'Optuna', 'Paddle', 'Pandas', 'Pocker', 'Pybrain', 'PyCaret',\n",
       "       'pycm', 'PyMC', 'Pyro', 'PyTensor', 'PyTorch',\n",
       "       'Pytorch image models', 'qlib', 'Rasa', 'Ray', 'Recommenders',\n",
       "       'Redash', 'river', 'RLAgents', 'scikit-learn', 'Scikit-multiflow',\n",
       "       'SciPy', 'Scrapy', 'seaborn', 'Snorkel', 'Sonnet', 'spaCy',\n",
       "       'Stable Diffusion webui', 'Stanza', 'Statsmodels', 'SuperAGI',\n",
       "       'TensorFlow', 'TensorForce', 'TensorLayer', 'TensorLayerX',\n",
       "       'torchvision', 'TPOT', 'Transformers', 'TTS', 'vit-pytorch',\n",
       "       'Yellowbrick'], dtype=object)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 38)\n",
      "(21, 38)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determining the minimal subset of metrics on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammon's error function that we will use as an objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sammon's error function\n",
    "def sammons_error(X, Y):\n",
    "    \"\"\"\n",
    "    X: Original high-dimensional data\n",
    "    Y: Low-dimensional representation\n",
    "    \"\"\"\n",
    "    # Calculate pairwise distances in X and Y\n",
    "    dist_orig = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "    dist_lowd = np.sqrt(np.sum((Y[:, np.newaxis, :] - Y[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "\n",
    "    # Avoid division by zero\n",
    "    epsilon = 1e-12\n",
    "\n",
    "    numerator = np.sum(((dist_orig - dist_lowd) * 2) / (dist_orig + epsilon))\n",
    "    denominator = np.sum(dist_orig)\n",
    "\n",
    "    error = numerator / denominator\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_genetic(individual, X):\n",
    "    # Create a subset of features based on the individual\n",
    "    subset_X = X[:, np.array(individual).astype(bool)]\n",
    "\n",
    "    # Calculate the fitness using the `sammons_error` function\n",
    "    fitness = -sammons_error(X, subset_X)\n",
    "    return fitness,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects giperion\\lsd-metrics\\venv\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "d:\\Projects giperion\\lsd-metrics\\venv\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "from deap import creator, base, tools, algorithms\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up the genetic algorithm\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "\n",
    "# Functions to limit the number of features in the individual to 10\n",
    "def create_individual():\n",
    "    individual = [random.randint(0, 1) for _ in range(X.shape[1])]\n",
    "    while sum(individual) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(individual) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            individual[idx] = 0\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    child1, child2 = [toolbox.clone(ind) for ind in (ind1, ind2)]\n",
    "    tools.cxTwoPoint(child1, child2)\n",
    "\n",
    "    # Limit the number of features in the child to 10\n",
    "    while sum(child1) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child1) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            child1[idx] = 0\n",
    "    while sum(child2) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child2) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            child2[idx] = 0\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def mutate(individual):\n",
    "    child = toolbox.clone(individual)\n",
    "    tools.mutFlipBit(child, indpb=0.05)\n",
    "    while sum(child) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            child[idx] = 0\n",
    "    return child,\n",
    "\n",
    "\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", fitness_genetic, X=X_train)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg         \tstd        \tmin         \tmax         \n",
      "0  \t100   \t-3.12644e-06\t1.47571e-06\t-5.46264e-06\t-4.25617e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  \t69    \t-1.78717e-06\t1.13958e-06\t-5.16721e-06\t-3.24598e-07\n",
      "2  \t56    \t-9.33332e-07\t6.20259e-07\t-4.20687e-06\t-2.73347e-07\n",
      "3  \t57    \t-6.47685e-07\t4.2254e-07 \t-2.2848e-06 \t-1.68969e-07\n",
      "4  \t64    \t-5.1643e-07 \t3.17264e-07\t-1.97969e-06\t-1.15977e-07\n",
      "5  \t64    \t-5.01358e-07\t4.9658e-07 \t-3.71884e-06\t-1.15977e-07\n",
      "6  \t69    \t-4.05469e-07\t3.9689e-07 \t-2.01889e-06\t-1.15977e-07\n",
      "7  \t65    \t-3.45587e-07\t4.84381e-07\t-3.81399e-06\t-9.17755e-08\n",
      "8  \t66    \t-2.6964e-07 \t4.39743e-07\t-3.31674e-06\t-5.2306e-08 \n",
      "9  \t63    \t-2.2918e-07 \t3.26902e-07\t-1.91052e-06\t-5.2306e-08 \n",
      "10 \t53    \t-1.63985e-07\t2.62122e-07\t-2.01889e-06\t-5.2306e-08 \n",
      "11 \t57    \t-2.03499e-07\t3.10396e-07\t-1.9169e-06 \t-5.2306e-08 \n",
      "12 \t71    \t-2.51007e-07\t4.97414e-07\t-3.67494e-06\t-5.20341e-08\n",
      "13 \t61    \t-1.75586e-07\t2.92552e-07\t-1.6986e-06 \t-5.2306e-08 \n",
      "14 \t63    \t-1.91977e-07\t3.53366e-07\t-1.69162e-06\t-4.80154e-08\n",
      "15 \t57    \t-1.50695e-07\t3.97679e-07\t-3.37067e-06\t-2.71188e-08\n",
      "16 \t51    \t-1.31402e-07\t2.54136e-07\t-1.46413e-06\t-2.71188e-08\n",
      "17 \t53    \t-1.00273e-07\t2.03093e-07\t-1.79089e-06\t-2.71188e-08\n",
      "18 \t65    \t-2.31926e-07\t5.34109e-07\t-3.74353e-06\t-2.71188e-08\n",
      "19 \t73    \t-8.50352e-08\t1.70867e-07\t-1.24382e-06\t-2.71188e-08\n",
      "20 \t56    \t-1.59812e-07\t3.70422e-07\t-2.11392e-06\t-2.71188e-08\n",
      "21 \t59    \t-1.28867e-07\t3.3917e-07 \t-1.79179e-06\t-2.71188e-08\n",
      "22 \t58    \t-1.27638e-07\t3.17414e-07\t-1.83446e-06\t-2.6992e-08 \n",
      "23 \t53    \t-1.06448e-07\t3.00873e-07\t-1.78539e-06\t-2.6992e-08 \n",
      "24 \t62    \t-1.56035e-07\t4.25898e-07\t-2.0646e-06 \t-2.6992e-08 \n",
      "25 \t57    \t-8.47421e-08\t2.23074e-07\t-1.59142e-06\t-2.6992e-08 \n",
      "26 \t62    \t-1.11556e-07\t3.17529e-07\t-1.85092e-06\t-2.6992e-08 \n",
      "27 \t54    \t-1.35932e-07\t4.26161e-07\t-3.19629e-06\t-2.6992e-08 \n",
      "28 \t63    \t-9.53533e-08\t2.79671e-07\t-2.33447e-06\t-2.30384e-08\n",
      "29 \t54    \t-7.25281e-08\t1.7667e-07 \t-1.14209e-06\t-2.30384e-08\n",
      "30 \t53    \t-7.83752e-08\t2.1642e-07 \t-1.55997e-06\t-2.30384e-08\n",
      "31 \t51    \t-8.48314e-08\t2.54888e-07\t-1.82972e-06\t-2.30384e-08\n",
      "32 \t49    \t-1.43602e-07\t3.49405e-07\t-1.60615e-06\t-2.07587e-08\n",
      "33 \t62    \t-1.40067e-07\t4.34176e-07\t-3.23452e-06\t-2.30384e-08\n",
      "34 \t57    \t-1.6711e-07 \t3.80561e-07\t-1.79466e-06\t-2.30384e-08\n",
      "35 \t53    \t-6.55313e-08\t1.85869e-07\t-1.60064e-06\t-2.30384e-08\n",
      "36 \t62    \t-1.17943e-07\t3.15093e-07\t-1.79483e-06\t-2.30384e-08\n",
      "37 \t62    \t-1.54513e-07\t4.1599e-07 \t-1.85092e-06\t-2.30384e-08\n",
      "38 \t65    \t-5.17098e-08\t1.57198e-07\t-1.54629e-06\t-2.30384e-08\n",
      "39 \t67    \t-1.07701e-07\t2.89644e-07\t-1.65953e-06\t-2.30384e-08\n",
      "40 \t58    \t-5.84713e-08\t1.83799e-07\t-1.55457e-06\t-2.30384e-08\n",
      "41 \t67    \t-1.58933e-07\t3.68787e-07\t-1.78563e-06\t-2.30384e-08\n",
      "42 \t66    \t-1.13041e-07\t3.09779e-07\t-1.64485e-06\t-2.30384e-08\n",
      "43 \t58    \t-7.83898e-08\t2.44877e-07\t-1.83733e-06\t-2.30384e-08\n",
      "44 \t55    \t-1.08907e-07\t2.74588e-07\t-1.59424e-06\t-2.30384e-08\n",
      "45 \t53    \t-5.84609e-08\t2.09254e-07\t-1.88609e-06\t-2.30384e-08\n",
      "46 \t65    \t-1.29551e-07\t4.43133e-07\t-3.68027e-06\t-2.30384e-08\n",
      "47 \t60    \t-1.42815e-07\t3.50733e-07\t-1.78559e-06\t-2.30384e-08\n",
      "48 \t64    \t-1.41771e-07\t4.30115e-07\t-3.26275e-06\t-2.30384e-08\n",
      "49 \t54    \t-1.07469e-07\t2.86254e-07\t-1.77657e-06\t-2.30384e-08\n",
      "50 \t47    \t-9.54657e-08\t2.58717e-07\t-1.55457e-06\t-2.30384e-08\n",
      "51 \t61    \t-7.15883e-08\t2.0837e-07 \t-1.64534e-06\t-2.30384e-08\n",
      "52 \t59    \t-7.8052e-08 \t2.63031e-07\t-2.11505e-06\t-2.30384e-08\n",
      "53 \t57    \t-1.17462e-07\t3.11501e-07\t-1.64529e-06\t-2.30384e-08\n",
      "54 \t58    \t-7.5903e-08 \t2.34732e-07\t-1.55457e-06\t-2.30384e-08\n",
      "55 \t53    \t-1.15712e-07\t3.19617e-07\t-1.63857e-06\t-2.30384e-08\n",
      "56 \t62    \t-9.4175e-08 \t2.86409e-07\t-2.0698e-06 \t-2.30384e-08\n",
      "57 \t52    \t-1.13268e-07\t3.31304e-07\t-1.78879e-06\t-2.30384e-08\n",
      "58 \t63    \t-5.76212e-08\t1.97265e-07\t-1.5545e-06 \t-2.30384e-08\n",
      "59 \t54    \t-1.16262e-07\t3.60961e-07\t-1.87834e-06\t-2.30384e-08\n",
      "60 \t44    \t-7.86315e-08\t2.35267e-07\t-1.55457e-06\t-2.30384e-08\n",
      "61 \t64    \t-1.43684e-07\t3.60146e-07\t-1.83346e-06\t-2.30384e-08\n",
      "62 \t66    \t-9.12855e-08\t2.54517e-07\t-1.60372e-06\t-2.30384e-08\n",
      "63 \t51    \t-1.35427e-07\t4.20064e-07\t-3.18718e-06\t-2.30384e-08\n",
      "64 \t59    \t-1.18091e-07\t3.27306e-07\t-1.78879e-06\t-2.30384e-08\n",
      "65 \t56    \t-1.01967e-07\t2.71566e-07\t-1.61484e-06\t-2.30384e-08\n",
      "66 \t64    \t-1.3099e-07 \t3.7609e-07 \t-1.86383e-06\t-2.30384e-08\n",
      "67 \t68    \t-1.05997e-07\t3.08658e-07\t-1.55452e-06\t-2.30384e-08\n",
      "68 \t55    \t-7.63928e-08\t2.51307e-07\t-1.82878e-06\t-2.30384e-08\n",
      "69 \t58    \t-1.72462e-07\t4.20625e-07\t-2.09202e-06\t-2.30384e-08\n",
      "70 \t62    \t-1.18021e-07\t3.19238e-07\t-1.60392e-06\t-2.30384e-08\n",
      "71 \t60    \t-1.19009e-07\t3.5444e-07 \t-2.10252e-06\t-2.30384e-08\n",
      "72 \t65    \t-8.97121e-08\t2.16813e-07\t-1.15539e-06\t-2.30384e-08\n",
      "73 \t47    \t-9.68259e-08\t3.8957e-07 \t-3.18718e-06\t-2.30384e-08\n",
      "74 \t66    \t-9.7919e-08 \t2.96511e-07\t-1.58557e-06\t-2.30384e-08\n",
      "75 \t53    \t-8.26872e-08\t2.07321e-07\t-1.55457e-06\t-2.30384e-08\n",
      "76 \t60    \t-7.94087e-08\t2.12323e-07\t-1.60355e-06\t-2.30384e-08\n",
      "77 \t63    \t-5.25964e-08\t1.16725e-07\t-9.51696e-07\t-2.30384e-08\n",
      "78 \t72    \t-1.88922e-07\t4.23105e-07\t-1.78878e-06\t-2.30384e-08\n",
      "79 \t66    \t-1.17346e-07\t3.18812e-07\t-1.82025e-06\t-2.30384e-08\n",
      "80 \t63    \t-1.02648e-07\t3.05503e-07\t-1.78879e-06\t-2.30384e-08\n",
      "81 \t56    \t-8.2082e-08 \t2.12801e-07\t-1.55456e-06\t-2.30384e-08\n",
      "82 \t57    \t-1.5045e-07 \t4.01695e-07\t-1.80777e-06\t-2.30384e-08\n",
      "83 \t58    \t-1.12502e-07\t4.01553e-07\t-3.18718e-06\t-2.30384e-08\n",
      "84 \t67    \t-1.09573e-07\t2.80155e-07\t-1.82878e-06\t-2.30384e-08\n",
      "85 \t57    \t-1.47655e-07\t3.49179e-07\t-1.76526e-06\t-2.30384e-08\n",
      "86 \t58    \t-1.37601e-07\t3.51608e-07\t-1.78878e-06\t-2.30384e-08\n",
      "87 \t68    \t-1.5083e-07 \t3.93926e-07\t-1.86343e-06\t-2.30384e-08\n",
      "88 \t61    \t-7.87628e-08\t2.52293e-07\t-1.82871e-06\t-1.18564e-08\n",
      "89 \t57    \t-1.53818e-07\t4.11807e-07\t-1.89052e-06\t-1.18564e-08\n",
      "90 \t56    \t-8.88824e-08\t2.57241e-07\t-1.78485e-06\t-1.18564e-08\n",
      "91 \t63    \t-7.37923e-08\t2.01162e-07\t-1.19509e-06\t-1.18564e-08\n",
      "92 \t60    \t-1.15852e-07\t3.47306e-07\t-1.59053e-06\t-1.18564e-08\n",
      "93 \t55    \t-9.04589e-08\t2.65415e-07\t-1.54131e-06\t-1.18564e-08\n",
      "94 \t55    \t-9.66838e-08\t3.79237e-07\t-3.16914e-06\t-1.18564e-08\n",
      "95 \t68    \t-8.22179e-08\t2.69936e-07\t-1.59033e-06\t-1.18564e-08\n",
      "96 \t53    \t-5.08405e-08\t1.56916e-07\t-1.14198e-06\t-1.18564e-08\n",
      "97 \t64    \t-8.124e-08  \t2.57898e-07\t-1.81337e-06\t-1.18564e-08\n",
      "98 \t63    \t-9.06022e-08\t2.68679e-07\t-1.54131e-06\t-1.18564e-08\n",
      "99 \t65    \t-1.1761e-07 \t3.54356e-07\t-1.59143e-06\t-1.18564e-08\n",
      "100\t54    \t-9.69415e-08\t2.48961e-07\t-1.18939e-06\t-1.18564e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Run the genetic algorithm\n",
    "population = toolbox.population(n=100)\n",
    "hof = tools.HallOfFame(100)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "population, log = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=100, stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset of metrics: ['effort', 'CEE', 'LDC', 'LLDC', 'TLLOC', 'TLOC', 'TNOS', 'TCLOC', 'WarningMajor', 'WarningMinor']\n",
      "Best fitness: (-1.1856353533101324e-08,)\n",
      "Length of best subset: 10\n"
     ]
    }
   ],
   "source": [
    "# Get the best individual (subset of metrics)\n",
    "best_individual = hof.items[0]\n",
    "best_subset_genetic = [df.columns[i+1] for i, bit in enumerate(best_individual) if bit]\n",
    "\n",
    "print(\"Best subset of metrics:\", best_subset_genetic)\n",
    "print(\"Best fitness:\", fitness_genetic(best_individual, X_train))\n",
    "print(\"Length of best subset:\", len(best_subset_genetic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset: [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
      " 0]\n",
      "Best sammon's error result: 1.6922541850323575e-06\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def fitness_annealing(X, subset_X):\n",
    "    return -sammons_error(X, subset_X)\n",
    "\n",
    "def accept_probability(curr_score, best_score, temperature):\n",
    "    if curr_score > best_score:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return math.exp((curr_score - best_score) / temperature)\n",
    "    \n",
    "def generate_random_subset(X):\n",
    "    # Generate new random subset\n",
    "    subset = np.random.randint(2, size=(38,)) # 0 or 1 masks\n",
    "\n",
    "    # Limit the number of features in the subset to 10\n",
    "    while sum(subset) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(subset) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            subset[idx] = 0\n",
    "\n",
    "    return subset\n",
    "\n",
    "def annealing(X):\n",
    "    # Simulated annealing parameters\n",
    "    t = 10000 # Initial temperature\n",
    "    t_min = 1e-3 # Minimum temperature\n",
    "    cooling_factor = 0.9990 # Temperature damping factor\n",
    "    max_iterations = 10000 # Maximum number of iterations\n",
    "    best_subset = None\n",
    "    best_score = float(\"-inf\")\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "\n",
    "        subset = generate_random_subset(X) # Generate random subset    \n",
    "\n",
    "        score = fitness_annealing(X, X[:,subset==1]) # Evaluate subset\n",
    "\n",
    "        # Update best, higher is better\n",
    "        if score > best_score or random.random() < accept_probability(score, best_score, t):\n",
    "            best_subset = subset\n",
    "            best_score = score\n",
    "\n",
    "        # Cool temperature\n",
    "        t = t * cooling_factor\n",
    "\n",
    "        # Check if cooled enough\n",
    "        if t < t_min:\n",
    "            break\n",
    "\n",
    "    return best_subset, best_score\n",
    "\n",
    "best_subset, best_score = annealing(X_train)\n",
    "\n",
    "print(\"Best subset:\", best_subset)\n",
    "print(\"Best sammon's error result:\", -best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['effort', 'bugs', 'CCO', 'LDC', 'CLLC', 'CEG', 'TLOC', 'TNM', 'TNPKG', 'WarningMinor']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "best_subset_annealing = [df.columns[i+1] for i, bit in enumerate(best_subset) if bit]\n",
    "print(best_subset_annealing)\n",
    "print(len(best_subset_annealing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic algorithm result:  ['effort', 'bugs', 'CCO', 'LDC', 'CLLC', 'CEG', 'TLOC', 'TNM', 'TNPKG', 'WarningMinor']\n",
      "Simulated annealing result: ['effort', 'CEE', 'LDC', 'LLDC', 'TLLOC', 'TLOC', 'TNOS', 'TCLOC', 'WarningMajor', 'WarningMinor']\n",
      "Genetic training score: 1.1856353533101324e-08\n",
      "Annealing training score: 1.6922541850323575e-06\n",
      "Genetic validation score: 1.2196494464090206e-08\n",
      "Annealing validation score: 1.5579669309100993e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Genetic algorithm result: \", best_subset_annealing)\n",
    "print(\"Simulated annealing result:\", best_subset_genetic)\n",
    "\n",
    "X_genetic = X_train[:, np.array(best_individual).astype(bool)]\n",
    "X_annealing = X_train[:, np.array(best_subset).astype(bool)]\n",
    "\n",
    "X_genetic_test = X_test[:, np.array(best_individual).astype(bool)]\n",
    "X_annealing_test = X_test[:, np.array(best_subset).astype(bool)]\n",
    "\n",
    "print(\"Genetic training score:\", sammons_error(X_train, X_genetic))\n",
    "print(\"Annealing training score:\", sammons_error(X_train, X_annealing))\n",
    "\n",
    "print(\"Genetic validation score:\", sammons_error(X_test, X_genetic_test))\n",
    "print(\"Annealing validation score:\", sammons_error(X_test, X_annealing_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
