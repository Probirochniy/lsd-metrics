{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 6\n",
    "\n",
    "## Running the experiments\n",
    "\n",
    "In the previous stage, our team selected `Sammon's error` function and `Genetic algorithm` and `Simulated annealing` for the experiments, so we wil use them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('combined-metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Name', axis=1).values\n",
    "y = df['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 38)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42625369e+00, 3.77802360e+01, 2.97935103e+01, ...,\n",
       "        1.81400000e+03, 1.58500000e+04, 0.00000000e+00],\n",
       "       [2.02732240e+00, 9.52868852e+00, 6.37568306e+00, ...,\n",
       "        9.22000000e+02, 2.64500000e+03, 0.00000000e+00],\n",
       "       [1.67987805e+00, 1.17048780e+01, 8.17073171e+00, ...,\n",
       "        2.79100000e+03, 1.14790000e+04, 0.00000000e+00],\n",
       "       ...,\n",
       "       [4.29198473e+00, 2.41488550e+01, 1.87232824e+01, ...,\n",
       "        3.82000000e+02, 6.43600000e+03, 0.00000000e+00],\n",
       "       [9.15789474e+00, 3.77017544e+01, 2.96842105e+01, ...,\n",
       "        1.78000000e+02, 1.38800000e+03, 0.00000000e+00],\n",
       "       [2.24528302e+00, 1.04371069e+01, 7.09119497e+00, ...,\n",
       "        8.01000000e+02, 2.98200000e+03, 0.00000000e+00]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ART', 'BentoML', 'Bokeh', 'Camel', 'CatBoost', 'Causal ML',\n",
       "       'Chainer', 'Computer Vision', 'D2L', 'Darts', 'Dash', 'DeepChem',\n",
       "       'DeepMind Control', 'DeepPavlov', 'Detectron2', 'DIGITS',\n",
       "       'DragGan', 'EasyOCR', 'ELI5', 'EvalAI', 'facenet', 'FaceSwap',\n",
       "       'Fairseq', 'FastAI', 'FeatureTools', 'FiftyOne', 'gensim',\n",
       "       'Giskard', 'Gluonts', 'Google Flax', 'Google JAX', 'GPT-Engineer',\n",
       "       'GPTDiscord', 'Gradio', 'Gymnasium', 'Horovod', 'ImageAI',\n",
       "       'imbalanced-learn', 'InsightFace', 'Kaolin', 'Kedro', 'Keras',\n",
       "       'Kserve', 'Lightning', 'Ludwig', 'Mage-ai', 'Mars', 'Matplotlib',\n",
       "       'metatransformer', 'Mindsdb', 'MLflow', 'Mycroft',\n",
       "       'Neural Prophet', 'NNI', 'Numpy', 'ONNX', 'Open-Assistant',\n",
       "       'OpenAI Baselines', 'OpenAI Python API library', 'OpenVINO',\n",
       "       'Optuna', 'Paddle', 'Pandas', 'Pocker', 'Pybrain', 'PyCaret',\n",
       "       'pycm', 'PyMC', 'Pyro', 'PyTensor', 'PyTorch',\n",
       "       'Pytorch image models', 'qlib', 'Rasa', 'Ray', 'Recommenders',\n",
       "       'Redash', 'river', 'RLAgents', 'scikit-learn', 'Scikit-multiflow',\n",
       "       'SciPy', 'Scrapy', 'seaborn', 'Snorkel', 'Sonnet', 'spaCy',\n",
       "       'Stable Diffusion webui', 'Stanza', 'Statsmodels', 'SuperAGI',\n",
       "       'TensorFlow', 'TensorForce', 'TensorLayer', 'TensorLayerX',\n",
       "       'torchvision', 'TPOT', 'Transformers', 'TTS', 'vit-pytorch',\n",
       "       'Yellowbrick'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 38)\n",
      "(21, 38)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determining the minimal subset of metrics on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammon's error function that we will use as an objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sammon's error function\n",
    "def sammons_error(X, Y):\n",
    "    \"\"\"\n",
    "    X: Original high-dimensional data\n",
    "    Y: Low-dimensional representation\n",
    "    \"\"\"\n",
    "    # Calculate pairwise distances in X and Y\n",
    "    dist_orig = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "    dist_lowd = np.sqrt(np.sum((Y[:, np.newaxis, :] - Y[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "\n",
    "    # Avoid division by zero\n",
    "    epsilon = 1e-12\n",
    "\n",
    "    numerator = np.sum(((dist_orig - dist_lowd) * 2) / (dist_orig + epsilon))\n",
    "    denominator = np.sum(dist_orig)\n",
    "\n",
    "    error = numerator / denominator\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_genetic(individual, X):\n",
    "    # Create a subset of features based on the individual\n",
    "    subset_X = X[:, np.array(individual).astype(bool)]\n",
    "\n",
    "    # Calculate the fitness using the `sammons_error` function\n",
    "    fitness = -sammons_error(X, subset_X)\n",
    "    return fitness,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import creator, base, tools, algorithms\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up the genetic algorithm\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "\n",
    "# Functions to limit the number of features in the individual to 25\n",
    "def create_individual():\n",
    "    individual = [random.randint(0, 1) for _ in range(X.shape[1])]\n",
    "    while sum(individual) > 25:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(individual) - 25, replace=False)\n",
    "        for idx in indices:\n",
    "            individual[idx] = 0\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    child1, child2 = [toolbox.clone(ind) for ind in (ind1, ind2)]\n",
    "    tools.cxTwoPoint(child1, child2)\n",
    "\n",
    "    # Limit the number of features in the child to 25\n",
    "    while sum(child1) > 25:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child1) - 25, replace=False)\n",
    "        for idx in indices:\n",
    "            child1[idx] = 0\n",
    "    while sum(child2) > 25:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child2) - 25, replace=False)\n",
    "        for idx in indices:\n",
    "            child2[idx] = 0\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def mutate(individual):\n",
    "    child = toolbox.clone(individual)\n",
    "    tools.mutFlipBit(child, indpb=0.05)\n",
    "    while sum(child) > 25:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child) - 25, replace=False)\n",
    "        for idx in indices:\n",
    "            child[idx] = 0\n",
    "    return child,\n",
    "\n",
    "\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", fitness_genetic, X=X_train)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg         \tstd        \tmin         \tmax         \n",
      "0  \t100   \t-1.93929e-06\t1.39794e-06\t-5.01034e-06\t-7.44105e-08\n",
      "1  \t66    \t-7.52009e-07\t5.98858e-07\t-2.45842e-06\t-5.11421e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  \t63    \t-3.21586e-07\t3.01651e-07\t-2.04449e-06\t-5.09807e-08\n",
      "3  \t65    \t-1.93544e-07\t2.01597e-07\t-1.8389e-06 \t-1.56846e-08\n",
      "4  \t58    \t-1.33633e-07\t2.55222e-07\t-1.86428e-06\t-1.56846e-08\n",
      "5  \t65    \t-6.81563e-08\t6.17763e-08\t-3.2524e-07 \t-4.67344e-10\n",
      "6  \t61    \t-6.14201e-08\t1.53245e-07\t-1.20946e-06\t-4.61628e-10\n",
      "7  \t55    \t-7.48719e-08\t2.66823e-07\t-1.79787e-06\t-4.61628e-10\n",
      "8  \t55    \t-5.87705e-08\t2.48812e-07\t-1.53183e-06\t-4.61628e-10\n",
      "9  \t59    \t-5.00508e-08\t2.3234e-07 \t-1.53164e-06\t-3.08197e-10\n",
      "10 \t67    \t-9.44696e-09\t4.29224e-08\t-3.24233e-07\t-4.57732e-10\n",
      "11 \t53    \t-7.5635e-08 \t4.18026e-07\t-3.78036e-06\t-1.68474e-10\n",
      "12 \t72    \t-4.85894e-08\t2.36694e-07\t-1.5596e-06 \t-1.9908e-10 \n",
      "13 \t60    \t-5.39993e-08\t2.61567e-07\t-1.53182e-06\t-1.9908e-10 \n",
      "14 \t54    \t-4.23305e-08\t2.21427e-07\t-1.79753e-06\t-1.98518e-10\n",
      "15 \t61    \t-6.81634e-08\t2.89012e-07\t-1.52602e-06\t-3.96452e-11\n",
      "16 \t63    \t-3.38642e-08\t1.59259e-07\t-1.52579e-06\t-3.96452e-11\n",
      "17 \t63    \t-7.55579e-08\t2.66544e-07\t-1.52633e-06\t-3.96452e-11\n",
      "18 \t57    \t-7.68726e-08\t3.01225e-07\t-1.52663e-06\t-8.33068e-12\n",
      "19 \t53    \t-7.0755e-08 \t2.91069e-07\t-1.75545e-06\t-8.33068e-12\n",
      "20 \t64    \t-1.41483e-08\t4.54829e-08\t-2.19554e-07\t-8.33068e-12\n",
      "21 \t57    \t-8.53916e-08\t3.04323e-07\t-1.7972e-06 \t-4.9693e-12 \n",
      "22 \t70    \t-6.40424e-08\t2.75067e-07\t-1.52673e-06\t-4.42532e-12\n",
      "23 \t67    \t-8.24126e-08\t3.12141e-07\t-1.75455e-06\t-4.42532e-12\n",
      "24 \t63    \t-6.22667e-08\t2.65176e-07\t-1.559e-06  \t-4.42532e-12\n",
      "25 \t70    \t-4.7159e-08 \t2.1939e-07 \t-1.52611e-06\t-4.42532e-12\n",
      "26 \t50    \t-2.98445e-08\t1.7861e-07 \t-1.52575e-06\t-4.42532e-12\n",
      "27 \t58    \t-6.41729e-08\t2.36322e-07\t-1.22682e-06\t-4.42532e-12\n",
      "28 \t63    \t-9.40738e-08\t4.05047e-07\t-3.14321e-06\t-4.42532e-12\n",
      "29 \t60    \t-2.47942e-08\t1.20428e-07\t-1.12033e-06\t-4.11208e-12\n",
      "30 \t48    \t-4.978e-08  \t2.20522e-07\t-1.57919e-06\t-4.11208e-12\n",
      "31 \t58    \t-3.56134e-08\t1.9931e-07 \t-1.75676e-06\t-2.55171e-12\n",
      "32 \t54    \t-3.24077e-08\t1.84808e-07\t-1.58479e-06\t-2.55171e-12\n",
      "33 \t64    \t-6.46247e-08\t2.75262e-07\t-1.52606e-06\t-2.55171e-12\n",
      "34 \t56    \t-3.89402e-08\t2.19585e-07\t-1.79715e-06\t-2.55171e-12\n",
      "35 \t64    \t-7.32068e-08\t2.66295e-07\t-1.5444e-06 \t-2.22627e-12\n",
      "36 \t55    \t-8.41224e-08\t3.07116e-07\t-1.5264e-06 \t-2.22627e-12\n",
      "37 \t62    \t-3.18198e-08\t1.79042e-07\t-1.52575e-06\t-2.22627e-12\n",
      "38 \t55    \t-7.3348e-09 \t3.4263e-08 \t-2.49006e-07\t-2.22627e-12\n",
      "39 \t63    \t-6.04574e-08\t2.68723e-07\t-1.57949e-06\t-2.22627e-12\n",
      "40 \t53    \t-6.60926e-08\t2.67047e-07\t-1.52609e-06\t-2.22627e-12\n",
      "41 \t56    \t-2.57288e-08\t1.78454e-07\t-1.75675e-06\t-2.22627e-12\n",
      "42 \t58    \t-5.76881e-08\t2.7715e-07 \t-1.76037e-06\t-2.22627e-12\n",
      "43 \t56    \t-4.49858e-08\t2.35609e-07\t-1.57912e-06\t-2.22627e-12\n",
      "44 \t65    \t-6.63231e-08\t2.68739e-07\t-1.52574e-06\t-2.22627e-12\n",
      "45 \t64    \t-2.51441e-08\t1.62636e-07\t-1.57912e-06\t-2.22627e-12\n",
      "46 \t60    \t-4.54797e-08\t2.02885e-07\t-1.559e-06  \t-2.22627e-12\n",
      "47 \t64    \t-1.42593e-08\t9.53235e-08\t-9.25285e-07\t-2.22627e-12\n",
      "48 \t60    \t-2.91779e-08\t1.36928e-07\t-9.52961e-07\t-2.22627e-12\n",
      "49 \t60    \t-8.12282e-08\t2.85401e-07\t-1.75715e-06\t-2.22627e-12\n",
      "50 \t64    \t-3.11555e-08\t1.78608e-07\t-1.52616e-06\t-2.22627e-12\n",
      "51 \t53    \t-3.47733e-08\t1.39758e-07\t-9.53063e-07\t-2.22627e-12\n",
      "52 \t59    \t-3.32396e-08\t1.78767e-07\t-1.52574e-06\t-2.22627e-12\n",
      "53 \t58    \t-3.11094e-08\t1.96399e-07\t-1.57932e-06\t-2.22627e-12\n",
      "54 \t59    \t-4.33463e-08\t2.17926e-07\t-1.53194e-06\t-2.22627e-12\n",
      "55 \t73    \t-8.39629e-08\t3.24318e-07\t-1.559e-06  \t-2.22627e-12\n",
      "56 \t60    \t-8.7949e-08 \t3.36026e-07\t-1.80945e-06\t-2.22627e-12\n",
      "57 \t57    \t-5.66754e-08\t2.81873e-07\t-1.79715e-06\t-2.22627e-12\n",
      "58 \t67    \t-1.64047e-08\t9.77141e-08\t-9.25417e-07\t-2.22627e-12\n",
      "59 \t51    \t-4.72587e-09\t2.55562e-08\t-1.88777e-07\t-2.22627e-12\n",
      "60 \t53    \t-2.16562e-08\t1.04695e-07\t-9.74891e-07\t-2.22627e-12\n",
      "61 \t58    \t-1.18573e-07\t5.621e-07  \t-3.68015e-06\t-2.22627e-12\n",
      "62 \t49    \t-1.81747e-08\t9.78975e-08\t-9.25279e-07\t-2.22627e-12\n",
      "63 \t68    \t-5.32638e-08\t3.29197e-07\t-3.14319e-06\t-2.22627e-12\n",
      "64 \t60    \t-3.20676e-08\t1.79176e-07\t-1.5264e-06 \t-2.22627e-12\n",
      "65 \t69    \t-6.695e-08  \t2.94878e-07\t-1.53398e-06\t-2.22627e-12\n",
      "66 \t62    \t-3.76387e-08\t1.9932e-07 \t-1.52574e-06\t-2.22627e-12\n",
      "67 \t68    \t-7.30881e-09\t3.55593e-08\t-2.49011e-07\t-2.22627e-12\n",
      "68 \t54    \t-9.15373e-09\t4.89207e-08\t-4.16226e-07\t-2.22627e-12\n",
      "69 \t63    \t-3.89574e-08\t2.12019e-07\t-1.75431e-06\t-2.22627e-12\n",
      "70 \t65    \t-3.8747e-08 \t1.81787e-07\t-1.52574e-06\t-2.22627e-12\n",
      "71 \t63    \t-3.4969e-08 \t1.96036e-07\t-1.52574e-06\t-2.22627e-12\n",
      "72 \t58    \t-7.34813e-08\t2.66196e-07\t-1.52574e-06\t-2.22627e-12\n",
      "73 \t58    \t-7.09057e-09\t3.40436e-08\t-2.49006e-07\t-2.22627e-12\n",
      "74 \t46    \t-1.06131e-08\t9.218e-08  \t-9.25279e-07\t-2.22627e-12\n",
      "75 \t65    \t-2.40949e-08\t1.56416e-07\t-1.52574e-06\t-2.22627e-12\n",
      "76 \t64    \t-1.83559e-08\t9.82021e-08\t-9.25573e-07\t-2.22627e-12\n",
      "77 \t60    \t-3.88419e-08\t1.83294e-07\t-1.52575e-06\t-2.22627e-12\n",
      "78 \t56    \t-3.02317e-08\t1.78255e-07\t-1.52574e-06\t-2.22627e-12\n",
      "79 \t71    \t-2.00995e-08\t1.00946e-07\t-9.25279e-07\t-2.22627e-12\n",
      "80 \t58    \t-2.58435e-08\t1.33839e-07\t-9.32598e-07\t-2.22627e-12\n",
      "81 \t47    \t-4.11944e-08\t1.83863e-07\t-1.52574e-06\t-2.22627e-12\n",
      "82 \t70    \t-2.6571e-08 \t1.34263e-07\t-9.25336e-07\t-2.22627e-12\n",
      "83 \t55    \t-7.20029e-08\t2.97813e-07\t-1.75676e-06\t-2.22627e-12\n",
      "84 \t62    \t-8.75225e-08\t2.84229e-07\t-1.52574e-06\t-2.22627e-12\n",
      "85 \t70    \t-1.74293e-08\t9.79819e-08\t-9.25279e-07\t-2.22627e-12\n",
      "86 \t63    \t-1.83805e-08\t1.0045e-07 \t-9.25896e-07\t-2.22627e-12\n",
      "87 \t48    \t-8.64565e-09\t4.38973e-08\t-2.75331e-07\t-2.01371e-12\n",
      "88 \t61    \t-4.85593e-08\t2.33488e-07\t-1.52606e-06\t-2.22627e-12\n",
      "89 \t54    \t-4.59386e-08\t2.42754e-07\t-1.52644e-06\t-2.22627e-12\n",
      "90 \t53    \t-2.79005e-08\t1.34197e-07\t-9.25596e-07\t-2.22627e-12\n",
      "91 \t61    \t-3.41659e-08\t1.84518e-07\t-1.57475e-06\t-2.22627e-12\n",
      "92 \t38    \t-2.6255e-08 \t1.38462e-07\t-9.74892e-07\t-2.22627e-12\n",
      "93 \t57    \t-1.43219e-08\t9.54955e-08\t-9.30697e-07\t-2.22627e-12\n",
      "94 \t70    \t-4.19675e-08\t2.26263e-07\t-1.75675e-06\t-2.22627e-12\n",
      "95 \t56    \t-2.82555e-08\t1.79784e-07\t-1.55567e-06\t-2.22627e-12\n",
      "96 \t54    \t-1.91877e-08\t1.53419e-07\t-1.52574e-06\t-2.22627e-12\n",
      "97 \t68    \t-7.75961e-08\t2.99054e-07\t-1.65762e-06\t-2.22627e-12\n",
      "98 \t59    \t-3.62185e-08\t1.59874e-07\t-9.25336e-07\t-2.01371e-12\n",
      "99 \t60    \t-7.67616e-08\t3.15036e-07\t-1.79715e-06\t-2.01371e-12\n",
      "100\t60    \t-1.10378e-08\t4.4853e-08 \t-2.94988e-07\t-2.01371e-12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Run the genetic algorithm\n",
    "population = toolbox.population(n=100)\n",
    "hof = tools.HallOfFame(100)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "population, log = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=100, stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset of metrics: ['h2', 'N1', 'N2', 'length', 'volume', 'effort', 'time', 'CCL', 'CCO', 'CEE', 'CI', 'LDC', 'LLDC', 'TLLOC', 'TLOC', 'TNCL', 'TNFI', 'TNM', 'TNOS', 'TNPKG', 'TCLOC', 'TNDI', 'WarningCritical', 'WarningMajor', 'WarningMinor']\n",
      "Best fitness: (-2.013710327311463e-12,)\n",
      "Length of best subset: 25\n"
     ]
    }
   ],
   "source": [
    "# Get the best individual (subset of metrics)\n",
    "best_individual = hof.items[0]\n",
    "best_subset_genetic = [df.columns[i+1] for i, bit in enumerate(best_individual) if bit]\n",
    "\n",
    "print(\"Best subset of metrics:\", best_subset_genetic)\n",
    "print(\"Best fitness:\", fitness_genetic(best_individual, X_train))\n",
    "print(\"Length of best subset:\", len(best_subset_genetic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset: [1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
      " 0]\n",
      "Best sammon's error result: 4.197337155943921e-06\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def fitness_annealing(X, subset_X):\n",
    "    return -sammons_error(X, subset_X)\n",
    "\n",
    "def accept_probability(curr_score, best_score, temperature):\n",
    "    if curr_score > best_score:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return math.exp((curr_score - best_score) / temperature)\n",
    "    \n",
    "def generate_random_subset(X):\n",
    "    # Generate new random subset\n",
    "    subset = np.random.randint(2, size=(38,)) # 0 or 1 masks\n",
    "\n",
    "    # Limit the number of features in the subset to 25\n",
    "    while sum(subset) > 25:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(subset) - 25, replace=False)\n",
    "        for idx in indices:\n",
    "            subset[idx] = 0\n",
    "\n",
    "    return subset\n",
    "\n",
    "def annealing(X):\n",
    "    # Simulated annealing parameters\n",
    "    t = 1000 # Initial temperature\n",
    "    t_min = 1e-3 # Minimum temperature\n",
    "    cooling_factor = 0.99 # Temperature damping factor\n",
    "    max_iterations = 1000 # Maximum number of iterations\n",
    "    best_subset = None\n",
    "    best_score = float(\"-inf\")\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "\n",
    "        subset = generate_random_subset(X) # Generate random subset    \n",
    "\n",
    "        score = fitness_annealing(X, X[:,subset==1]) # Evaluate subset\n",
    "\n",
    "        # Update best, higher is better\n",
    "        if score > best_score or random.random() < accept_probability(score, best_score, t):\n",
    "            best_subset = subset\n",
    "            best_score = score\n",
    "\n",
    "        # Cool temperature\n",
    "        t = t * cooling_factor\n",
    "\n",
    "        # Check if cooled enough\n",
    "        if t < t_min:\n",
    "            break\n",
    "\n",
    "    return best_subset, best_score\n",
    "\n",
    "best_subset, best_score = annealing(X_train)\n",
    "\n",
    "print(\"Best subset:\", best_subset)\n",
    "print(\"Best sammon's error result:\", -best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h1', 'N1', 'vocabulary', 'difficulty', 'effort', 'bugs', 'MI', 'CC', 'CCO', 'CI', 'CLC', 'CR', 'TNCL', 'TNM', 'TNPKG', 'TCLOC', 'WarningBlocker', 'WarningMinor']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "best_subset_annealing = [df.columns[i+1] for i, bit in enumerate(best_subset) if bit]\n",
    "print(best_subset_annealing)\n",
    "print(len(best_subset_annealing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic algorithm result:  ['h1', 'N1', 'vocabulary', 'difficulty', 'effort', 'bugs', 'MI', 'CC', 'CCO', 'CI', 'CLC', 'CR', 'TNCL', 'TNM', 'TNPKG', 'TCLOC', 'WarningBlocker', 'WarningMinor']\n",
      "Simulated annealing result: ['h2', 'N1', 'N2', 'length', 'volume', 'effort', 'time', 'CCL', 'CCO', 'CEE', 'CI', 'LDC', 'LLDC', 'TLLOC', 'TLOC', 'TNCL', 'TNFI', 'TNM', 'TNOS', 'TNPKG', 'TCLOC', 'TNDI', 'WarningCritical', 'WarningMajor', 'WarningMinor']\n",
      "Genetic training score: 2.013710327311463e-12\n",
      "Annealing training score: 4.197337155943921e-06\n",
      "Genetic validation score: 2.9773385857308484e-12\n",
      "Annealing validation score: 3.2220047850834426e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Genetic algorithm result: \", best_subset_annealing)\n",
    "print(\"Simulated annealing result:\", best_subset_genetic)\n",
    "\n",
    "\n",
    "\n",
    "X_genetic = X_train[:, np.array(best_individual).astype(bool)]\n",
    "X_annealing = X_train[:, np.array(best_subset).astype(bool)]\n",
    "\n",
    "X_genetic_test = X_test[:, np.array(best_individual).astype(bool)]\n",
    "X_annealing_test = X_test[:, np.array(best_subset).astype(bool)]\n",
    "\n",
    "print(\"Genetic training score:\", sammons_error(X_train, X_genetic))\n",
    "print(\"Annealing training score:\", sammons_error(X_train, X_annealing))\n",
    "\n",
    "print(\"Genetic validation score:\", sammons_error(X_test, X_genetic_test))\n",
    "print(\"Annealing validation score:\", sammons_error(X_test, X_annealing_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
