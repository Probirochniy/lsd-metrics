{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 6\n",
    "\n",
    "## Running the experiments\n",
    "\n",
    "In the previous stage, our team selected `Sammon's error` function and `Genetic algorithm` and `Simulated annealing` for the experiments, so we wil use them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('combined-metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Name', axis=1).values\n",
    "y = df['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42625369e+00, 3.77802360e+01, 2.97935103e+01, ...,\n",
       "        1.81400000e+03, 1.58500000e+04, 0.00000000e+00],\n",
       "       [2.02732240e+00, 9.52868852e+00, 6.37568306e+00, ...,\n",
       "        9.22000000e+02, 2.64500000e+03, 0.00000000e+00],\n",
       "       [1.67987805e+00, 1.17048780e+01, 8.17073171e+00, ...,\n",
       "        2.79100000e+03, 1.14790000e+04, 0.00000000e+00],\n",
       "       ...,\n",
       "       [4.29198473e+00, 2.41488550e+01, 1.87232824e+01, ...,\n",
       "        3.82000000e+02, 6.43600000e+03, 0.00000000e+00],\n",
       "       [9.15789474e+00, 3.77017544e+01, 2.96842105e+01, ...,\n",
       "        1.78000000e+02, 1.38800000e+03, 0.00000000e+00],\n",
       "       [2.24528302e+00, 1.04371069e+01, 7.09119497e+00, ...,\n",
       "        8.01000000e+02, 2.98200000e+03, 0.00000000e+00]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ART', 'BentoML', 'Bokeh', 'Camel', 'CatBoost', 'Causal ML',\n",
       "       'Chainer', 'Computer Vision', 'D2L', 'Darts', 'Dash', 'DeepChem',\n",
       "       'DeepMind Control', 'DeepPavlov', 'Detectron2', 'DIGITS',\n",
       "       'DragGan', 'EasyOCR', 'ELI5', 'EvalAI', 'facenet', 'FaceSwap',\n",
       "       'Fairseq', 'FastAI', 'FeatureTools', 'FiftyOne', 'gensim',\n",
       "       'Giskard', 'Gluonts', 'Google Flax', 'Google JAX', 'GPT-Engineer',\n",
       "       'GPTDiscord', 'Gradio', 'Gymnasium', 'Horovod', 'ImageAI',\n",
       "       'imbalanced-learn', 'InsightFace', 'Kaolin', 'Kedro', 'Keras',\n",
       "       'Kserve', 'Lightning', 'Ludwig', 'Mage-ai', 'Mars', 'Matplotlib',\n",
       "       'metatransformer', 'Mindsdb', 'MLflow', 'Mycroft',\n",
       "       'Neural Prophet', 'NNI', 'Numpy', 'ONNX', 'Open-Assistant',\n",
       "       'OpenAI Baselines', 'OpenAI Python API library', 'OpenVINO',\n",
       "       'Optuna', 'Paddle', 'Pandas', 'Pocker', 'Pybrain', 'PyCaret',\n",
       "       'pycm', 'PyMC', 'Pyro', 'PyTensor', 'PyTorch',\n",
       "       'Pytorch image models', 'qlib', 'Rasa', 'Ray', 'Recommenders',\n",
       "       'Redash', 'river', 'RLAgents', 'scikit-learn', 'Scikit-multiflow',\n",
       "       'SciPy', 'Scrapy', 'seaborn', 'Snorkel', 'Sonnet', 'spaCy',\n",
       "       'Stable Diffusion webui', 'Stanza', 'Statsmodels', 'SuperAGI',\n",
       "       'TensorFlow', 'TensorForce', 'TensorLayer', 'TensorLayerX',\n",
       "       'torchvision', 'TPOT', 'Transformers', 'TTS', 'vit-pytorch',\n",
       "       'Yellowbrick'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 35)\n",
      "(21, 35)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determining the minimal subset of metrics on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammon's error function that we will use as an objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sammon's error function\n",
    "def sammons_error(X, Y):\n",
    "    \"\"\"\n",
    "    X: Original high-dimensional data\n",
    "    Y: Low-dimensional representation\n",
    "    \"\"\"\n",
    "    # Calculate pairwise distances in X and Y\n",
    "    dist_orig = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "    dist_lowd = np.sqrt(np.sum((Y[:, np.newaxis, :] - Y[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "\n",
    "    # Avoid division by zero\n",
    "    epsilon = 1e-12\n",
    "\n",
    "    numerator = np.sum(((dist_orig - dist_lowd) * 2) / (dist_orig + epsilon))\n",
    "    denominator = np.sum(dist_orig)\n",
    "\n",
    "    error = numerator / denominator\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_genetic(individual, X):\n",
    "    # Create a subset of features based on the individual\n",
    "    subset_X = X[:, np.array(individual).astype(bool)]\n",
    "\n",
    "    # Calculate the fitness using the `sammons_error` function\n",
    "    fitness = -sammons_error(X, subset_X)\n",
    "    return fitness,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects giperion\\lsd-metrics\\venv\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "d:\\Projects giperion\\lsd-metrics\\venv\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "from deap import creator, base, tools, algorithms\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up the genetic algorithm\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "\n",
    "# Functions to limit the number of features in the individual to 10\n",
    "def create_individual():\n",
    "    individual = [random.randint(0, 1) for _ in range(X.shape[1])]\n",
    "    while sum(individual) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(individual) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            individual[idx] = 0\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "\n",
    "def mate(ind1, ind2):\n",
    "    child1, child2 = [toolbox.clone(ind) for ind in (ind1, ind2)]\n",
    "    tools.cxTwoPoint(child1, child2)\n",
    "\n",
    "    # Limit the number of features in the child to 10\n",
    "    while sum(child1) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child1) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            child1[idx] = 0\n",
    "    while sum(child2) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child2) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            child2[idx] = 0\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def mutate(individual):\n",
    "    child = toolbox.clone(individual)\n",
    "    tools.mutFlipBit(child, indpb=0.05)\n",
    "    while sum(child) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(child) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            child[idx] = 0\n",
    "    return child,\n",
    "\n",
    "\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", fitness_genetic, X=X_train)\n",
    "toolbox.register(\"mate\", mate)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg         \tstd        \tmin         \tmax         \n",
      "0  \t100   \t-4.41726e-06\t2.12633e-06\t-7.12144e-06\t-2.59199e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  \t57    \t-3.08783e-06\t2.08875e-06\t-6.89482e-06\t-2.59199e-07\n",
      "2  \t67    \t-1.62987e-06\t1.59125e-06\t-6.45965e-06\t-2.47664e-07\n",
      "3  \t62    \t-8.37881e-07\t8.2284e-07 \t-4.39582e-06\t-2.13384e-07\n",
      "4  \t53    \t-6.77807e-07\t7.94533e-07\t-4.58439e-06\t-1.20072e-07\n",
      "5  \t66    \t-5.19481e-07\t7.09249e-07\t-3.87091e-06\t-1.20075e-07\n",
      "6  \t59    \t-4.60476e-07\t7.95388e-07\t-4.58411e-06\t-8.50291e-08\n",
      "7  \t55    \t-4.06907e-07\t7.11527e-07\t-3.51324e-06\t-8.50291e-08\n",
      "8  \t69    \t-4.20278e-07\t7.97879e-07\t-4.8137e-06 \t-6.22203e-08\n",
      "9  \t56    \t-2.88706e-07\t5.82562e-07\t-4.22824e-06\t-8.43835e-08\n",
      "10 \t63    \t-2.21304e-07\t5.09802e-07\t-4.03802e-06\t-8.43835e-08\n",
      "11 \t54    \t-2.90028e-07\t7.29204e-07\t-4.03802e-06\t-8.43835e-08\n",
      "12 \t55    \t-2.03239e-07\t4.92641e-07\t-3.72519e-06\t-8.43835e-08\n",
      "13 \t54    \t-2.42596e-07\t6.52346e-07\t-4.24775e-06\t-8.43716e-08\n",
      "14 \t61    \t-2.87025e-07\t7.76521e-07\t-4.0366e-06 \t-2.7398e-08 \n",
      "15 \t53    \t-2.02567e-07\t5.90093e-07\t-4.03639e-06\t-2.7398e-08 \n",
      "16 \t64    \t-1.70478e-07\t3.65089e-07\t-3.36972e-06\t-6.13184e-08\n",
      "17 \t61    \t-1.9132e-07 \t3.69831e-07\t-3.2954e-06 \t-2.72374e-08\n",
      "18 \t66    \t-2.25546e-07\t5.20041e-07\t-4.03659e-06\t-2.69533e-08\n",
      "19 \t63    \t-2.30368e-07\t5.62571e-07\t-3.31262e-06\t-1.78261e-08\n",
      "20 \t62    \t-2.824e-07  \t7.28777e-07\t-4.038e-06  \t-1.78261e-08\n",
      "21 \t63    \t-2.84324e-07\t7.40236e-07\t-3.69698e-06\t-1.31919e-08\n",
      "22 \t49    \t-1.85542e-07\t5.45101e-07\t-3.24292e-06\t-1.31919e-08\n",
      "23 \t67    \t-1.1385e-07 \t3.42543e-07\t-3.23476e-06\t-1.31919e-08\n",
      "24 \t56    \t-1.90649e-07\t7.18885e-07\t-5.1237e-06 \t-3.27877e-09\n",
      "25 \t64    \t-1.61361e-07\t6.44558e-07\t-4.12259e-06\t-3.27877e-09\n",
      "26 \t62    \t-1.27781e-07\t5.3236e-07 \t-4.29079e-06\t-3.27877e-09\n",
      "27 \t62    \t-9.73152e-08\t4.24467e-07\t-4.09228e-06\t-3.27877e-09\n",
      "28 \t60    \t-8.30026e-08\t3.46948e-07\t-3.24294e-06\t-3.27877e-09\n",
      "29 \t57    \t-1.43659e-07\t5.44914e-07\t-3.14979e-06\t-3.27877e-09\n",
      "30 \t62    \t-9.89954e-08\t4.50364e-07\t-3.12597e-06\t-3.27877e-09\n",
      "31 \t57    \t-9.08122e-08\t4.44237e-07\t-3.14977e-06\t-3.27877e-09\n",
      "32 \t64    \t-1.97761e-07\t6.80435e-07\t-3.94458e-06\t-3.27877e-09\n",
      "33 \t72    \t-1.44011e-07\t5.5263e-07 \t-3.25228e-06\t-3.27877e-09\n",
      "34 \t51    \t-1.44784e-07\t6.29528e-07\t-4.11787e-06\t-3.27877e-09\n",
      "35 \t62    \t-1.33934e-07\t5.40555e-07\t-3.15051e-06\t-3.27877e-09\n",
      "36 \t58    \t-1.12501e-07\t5.09852e-07\t-3.87557e-06\t-3.27877e-09\n",
      "37 \t49    \t-1.20493e-07\t5.48867e-07\t-4.35388e-06\t-3.27877e-09\n",
      "38 \t71    \t-4.68615e-08\t1.42774e-07\t-8.65491e-07\t-3.27877e-09\n",
      "39 \t63    \t-7.69693e-08\t3.28073e-07\t-3.12597e-06\t-3.27877e-09\n",
      "40 \t61    \t-7.76135e-08\t3.35747e-07\t-3.12597e-06\t-3.27877e-09\n",
      "41 \t58    \t-6.64225e-08\t3.39422e-07\t-3.2523e-06 \t-3.27877e-09\n",
      "42 \t56    \t-1.42185e-07\t5.44216e-07\t-3.15051e-06\t-3.27877e-09\n",
      "43 \t66    \t-9.18543e-08\t4.4468e-07 \t-3.12597e-06\t-3.27877e-09\n",
      "44 \t61    \t-1.80022e-07\t6.76063e-07\t-4.05545e-06\t-3.27877e-09\n",
      "45 \t60    \t-1.36134e-07\t5.91447e-07\t-3.87557e-06\t-3.27877e-09\n",
      "46 \t61    \t-1.09783e-07\t4.62783e-07\t-3.25228e-06\t-3.27877e-09\n",
      "47 \t60    \t-2.18456e-07\t7.96824e-07\t-4.05545e-06\t-3.27877e-09\n",
      "48 \t67    \t-2.15099e-07\t7.42069e-07\t-4.1297e-06 \t-3.27877e-09\n",
      "49 \t65    \t-1.22249e-07\t6.58723e-07\t-5.18519e-06\t-3.27877e-09\n",
      "50 \t62    \t-1.06163e-07\t4.82293e-07\t-3.59079e-06\t-3.27877e-09\n",
      "51 \t55    \t-1.05249e-07\t5.37173e-07\t-3.1799e-06 \t-3.27877e-09\n",
      "52 \t57    \t-1.30391e-07\t5.92857e-07\t-4.00974e-06\t-3.27877e-09\n",
      "53 \t59    \t-9.81066e-08\t4.47153e-07\t-3.12597e-06\t-3.27877e-09\n",
      "54 \t62    \t-1.2354e-07 \t5.7287e-07 \t-3.59066e-06\t-3.27877e-09\n",
      "55 \t59    \t-7.7372e-08 \t3.34837e-07\t-3.12597e-06\t-3.27877e-09\n",
      "56 \t67    \t-8.33974e-08\t3.36159e-07\t-3.12597e-06\t-3.27877e-09\n",
      "57 \t37    \t-2.78076e-08\t1.09228e-07\t-8.27888e-07\t-3.27877e-09\n",
      "58 \t65    \t-1.22311e-07\t5.97082e-07\t-3.90574e-06\t-3.27877e-09\n",
      "59 \t58    \t-1.78613e-07\t6.89002e-07\t-4.05545e-06\t-3.27877e-09\n",
      "60 \t56    \t-1.67171e-07\t6.31415e-07\t-3.34375e-06\t-3.27877e-09\n",
      "61 \t65    \t-2.81188e-08\t9.99936e-08\t-5.64707e-07\t-3.27877e-09\n",
      "62 \t58    \t-8.61008e-08\t4.49306e-07\t-3.20905e-06\t-3.27877e-09\n",
      "63 \t58    \t-1.81726e-07\t6.65021e-07\t-3.87469e-06\t-3.27877e-09\n",
      "64 \t62    \t-9.72718e-08\t4.53636e-07\t-3.26135e-06\t-3.27877e-09\n",
      "65 \t55    \t-1.29956e-07\t5.82942e-07\t-3.87557e-06\t-3.27877e-09\n",
      "66 \t63    \t-1.19913e-07\t5.58717e-07\t-3.28735e-06\t-3.27877e-09\n",
      "67 \t60    \t-1.48348e-07\t6.46649e-07\t-4.05545e-06\t-3.27877e-09\n",
      "68 \t59    \t-1.44475e-07\t5.79462e-07\t-3.73498e-06\t-3.27877e-09\n",
      "69 \t73    \t-1.36781e-07\t6.09807e-07\t-4.05447e-06\t-3.27877e-09\n",
      "70 \t64    \t-1.06686e-07\t5.33046e-07\t-3.12597e-06\t-3.27877e-09\n",
      "71 \t49    \t-1.60282e-07\t6.30221e-07\t-3.24294e-06\t-3.27877e-09\n",
      "72 \t57    \t-9.22258e-08\t4.08686e-07\t-3.87557e-06\t-3.27877e-09\n",
      "73 \t56    \t-1.27696e-07\t5.28252e-07\t-4.19499e-06\t-3.27877e-09\n",
      "74 \t65    \t-2.19021e-07\t6.97843e-07\t-3.25121e-06\t-3.27877e-09\n",
      "75 \t64    \t-8.31694e-08\t3.37484e-07\t-3.12596e-06\t-3.27877e-09\n",
      "76 \t55    \t-3.6543e-08 \t1.12455e-07\t-5.85835e-07\t-3.27877e-09\n",
      "77 \t71    \t-2.20679e-07\t7.91451e-07\t-3.75203e-06\t-3.27877e-09\n",
      "78 \t59    \t-1.34723e-07\t5.53359e-07\t-3.2523e-06 \t-3.27877e-09\n",
      "79 \t71    \t-1.42745e-07\t5.40353e-07\t-3.15051e-06\t-3.27877e-09\n",
      "80 \t63    \t-2.39329e-07\t7.54422e-07\t-3.26135e-06\t-3.27877e-09\n",
      "81 \t63    \t-1.77075e-07\t7.9537e-07 \t-4.61115e-06\t-3.27877e-09\n",
      "82 \t62    \t-8.58532e-08\t3.43917e-07\t-3.12596e-06\t-3.27877e-09\n",
      "83 \t60    \t-1.01671e-07\t4.58153e-07\t-3.25121e-06\t-3.27877e-09\n",
      "84 \t48    \t-1.30893e-07\t5.19855e-07\t-4.00891e-06\t-3.27877e-09\n",
      "85 \t57    \t-2.37223e-07\t8.14417e-07\t-3.98056e-06\t-3.27877e-09\n",
      "86 \t57    \t-9.79706e-08\t5.27511e-07\t-5.126e-06  \t-3.27877e-09\n",
      "87 \t61    \t-1.28833e-07\t5.84146e-07\t-3.90317e-06\t-3.27877e-09\n",
      "88 \t64    \t-1.40756e-07\t5.8437e-07 \t-3.89723e-06\t-3.27877e-09\n",
      "89 \t72    \t-1.06898e-07\t4.94306e-07\t-3.59066e-06\t-3.27877e-09\n",
      "90 \t62    \t-1.15492e-07\t4.55579e-07\t-3.15051e-06\t-3.27877e-09\n",
      "91 \t56    \t-1.01559e-07\t5.06488e-07\t-3.87467e-06\t-3.27877e-09\n",
      "92 \t57    \t-1.21975e-07\t5.80365e-07\t-3.87557e-06\t-3.27877e-09\n",
      "93 \t66    \t-1.20446e-07\t4.68502e-07\t-3.12495e-06\t-3.27877e-09\n",
      "94 \t67    \t-9.71328e-08\t3.46868e-07\t-3.17916e-06\t-3.27877e-09\n",
      "95 \t58    \t-1.04233e-07\t4.78708e-07\t-3.59066e-06\t-3.27877e-09\n",
      "96 \t64    \t-1.40844e-07\t5.50269e-07\t-3.24123e-06\t-3.27877e-09\n",
      "97 \t54    \t-3.2332e-07 \t9.16381e-07\t-3.87557e-06\t-3.27877e-09\n",
      "98 \t55    \t-1.04283e-07\t4.64325e-07\t-3.32876e-06\t-3.27877e-09\n",
      "99 \t64    \t-1.18564e-07\t5.37806e-07\t-3.16971e-06\t-3.27877e-09\n",
      "100\t63    \t-1.43131e-07\t5.99695e-07\t-4.02762e-06\t-3.27877e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Run the genetic algorithm\n",
    "population = toolbox.population(n=100)\n",
    "hof = tools.HallOfFame(100)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "population, log = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=100, stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset of metrics: ['effort', 'CEE', 'LLDC', 'TLLOC', 'TNM', 'TNOS', 'TCLOC', 'WarningCritical', 'WarningMajor', 'WarningMinor']\n",
      "Best fitness: (-3.2787695694992544e-09,)\n",
      "Length of best subset: 10\n"
     ]
    }
   ],
   "source": [
    "# Get the best individual (subset of metrics)\n",
    "best_individual = hof.items[0]\n",
    "best_subset_genetic = [df.columns[i+1] for i, bit in enumerate(best_individual) if bit]\n",
    "\n",
    "print(\"Best subset of metrics:\", best_subset_genetic)\n",
    "print(\"Best fitness:\", fitness_genetic(best_individual, X_train))\n",
    "print(\"Length of best subset:\", len(best_subset_genetic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1]\n",
      "Best sammon's error result: 1.2091227099838297e-06\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def fitness_annealing(X, subset_X):\n",
    "    return -sammons_error(X, subset_X)\n",
    "\n",
    "def accept_probability(curr_score, best_score, temperature):\n",
    "    if curr_score > best_score:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return math.exp((curr_score - best_score) / temperature)\n",
    "    \n",
    "def generate_random_subset(X):\n",
    "    # Generate new random subset\n",
    "    subset = np.random.randint(2, size=(35,)) # 0 or 1 masks\n",
    "\n",
    "    # Limit the number of features in the subset to 10\n",
    "    while sum(subset) > 10:\n",
    "        indices = np.random.choice(np.arange(X.shape[1]), size=sum(subset) - 10, replace=False)\n",
    "        for idx in indices:\n",
    "            subset[idx] = 0\n",
    "\n",
    "    return subset\n",
    "\n",
    "def annealing(X):\n",
    "    # Simulated annealing parameters\n",
    "    t = 10000 # Initial temperature\n",
    "    t_min = 1e-3 # Minimum temperature\n",
    "    cooling_factor = 0.9990 # Temperature damping factor\n",
    "    max_iterations = 10000 # Maximum number of iterations\n",
    "    best_subset = None\n",
    "    best_score = float(\"-inf\")\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "\n",
    "        subset = generate_random_subset(X) # Generate random subset    \n",
    "\n",
    "        score = fitness_annealing(X, X[:,subset==1]) # Evaluate subset\n",
    "\n",
    "        # Update best, higher is better\n",
    "        if score > best_score or random.random() < accept_probability(score, best_score, t):\n",
    "            best_subset = subset\n",
    "            best_score = score\n",
    "\n",
    "        # Cool temperature\n",
    "        t = t * cooling_factor\n",
    "\n",
    "        # Check if cooled enough\n",
    "        if t < t_min:\n",
    "            break\n",
    "\n",
    "    return best_subset, best_score\n",
    "\n",
    "best_subset, best_score = annealing(X_train)\n",
    "\n",
    "print(\"Best subset:\", best_subset)\n",
    "print(\"Best sammon's error result:\", -best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N1', 'CC', 'CCL', 'CEE', 'CLLC', 'NCR', 'TLLOC', 'WarningBlocker', 'WarningCritical', 'WarningInfo']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "best_subset_annealing = [df.columns[i+1] for i, bit in enumerate(best_subset) if bit]\n",
    "print(best_subset_annealing)\n",
    "print(len(best_subset_annealing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic algorithm result:  ['effort', 'CEE', 'LLDC', 'TLLOC', 'TNM', 'TNOS', 'TCLOC', 'WarningCritical', 'WarningMajor', 'WarningMinor']\n",
      "Simulated annealing result: ['N1', 'CC', 'CCL', 'CEE', 'CLLC', 'NCR', 'TLLOC', 'WarningBlocker', 'WarningCritical', 'WarningInfo']\n",
      "Genetic training score: 3.2787695694992544e-09\n",
      "Annealing training score: 1.2091227099838297e-06\n",
      "Genetic validation score: 1.3740122716132385e-08\n",
      "Annealing validation score: 1.0553328848265607e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Genetic algorithm result: \", best_subset_genetic)\n",
    "print(\"Simulated annealing result:\", best_subset_annealing)\n",
    "\n",
    "X_genetic = X_train[:, np.array(best_individual).astype(bool)]\n",
    "X_annealing = X_train[:, np.array(best_subset).astype(bool)]\n",
    "\n",
    "X_genetic_test = X_test[:, np.array(best_individual).astype(bool)]\n",
    "X_annealing_test = X_test[:, np.array(best_subset).astype(bool)]\n",
    "\n",
    "print(\"Genetic training score:\", sammons_error(X_train, X_genetic))\n",
    "print(\"Annealing training score:\", sammons_error(X_train, X_annealing))\n",
    "\n",
    "print(\"Genetic validation score:\", sammons_error(X_test, X_genetic_test))\n",
    "print(\"Annealing validation score:\", sammons_error(X_test, X_annealing_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
